<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>llm神经网络深度学习 | Johnny-Zhao's TechBlog</title><meta name="author" content="Johnny-Zhao"><meta name="copyright" content="Johnny-Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型">
<meta property="og:type" content="article">
<meta property="og:title" content="llm神经网络深度学习">
<meta property="og:url" content="https://zjncs.github.io/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Johnny-Zhao&#39;s TechBlog">
<meta property="og:description" content="下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zjncs.github.io/img/avatar-geek.jpg">
<meta property="article:published_time" content="2025-08-18T07:21:41.000Z">
<meta property="article:modified_time" content="2025-08-18T07:22:03.764Z">
<meta property="article:author" content="Johnny-Zhao">
<meta property="article:tag" content="2508">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zjncs.github.io/img/avatar-geek.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "llm神经网络深度学习",
  "url": "https://zjncs.github.io/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
  "image": "https://zjncs.github.io/img/avatar-geek.jpg",
  "datePublished": "2025-08-18T07:21:41.000Z",
  "dateModified": "2025-08-18T07:22:03.764Z",
  "author": [
    {
      "@type": "Person",
      "name": "Johnny-Zhao",
      "url": "https://github.com/zjncs"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-dark.ico"><link rel="canonical" href="https://zjncs.github.io/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#000000"/><link rel="apple-touch-icon" sizes="180x180" href="/img/pwa/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/pwa/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/pwa/16.png"/><link rel="mask-icon" href="/img/pwa/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'llm神经网络深度学习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/geek-theme.css"><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet"><style>
  :root {
    --geek-primary: #00ff7f;
    --geek-secondary: #1a1a1a;
    --geek-accent: #ff6b6b;
    --geek-bg: #0a0a0a;
    --geek-card: #1e1e1e;
    --geek-text: #e0e0e0;
    --geek-muted: #666666;
  }
</style>
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Johnny-Zhao's TechBlog" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Johnny-Zhao's TechBlog</span></a><a class="nav-page-title" href="/"><span class="site-name">llm神经网络深度学习</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">llm神经网络深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-18T07:21:41.000Z" title="发表于 2025-08-18 15:21:41">2025-08-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-18T07:22:03.764Z" title="更新于 2025-08-18 15:22:03">2025-08-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/llm/">llm+</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:365,&quot;messagePrev&quot;:&quot;[DEPRECATED] This post was last updated&quot;,&quot;messageNext&quot;:&quot;days ago. Content may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2025-08-18 15:22:03&quot;}" hidden></div><p><strong>下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做 Grid Search（网格搜索）。</strong></p>
<p>**　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如 EasyPR 字符识别网络架构（下图）。**<img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/1d3e42d157b2d6714982495a97feb882.png" alt="img"></p>
<p><strong>下面简单介绍一下两层神经网络的训练。</strong></p>
<p>**　　在 Rosenblat 提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000 左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。**</p>
<p>**　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为 yp，真实目标为 y。那么，定义一个值 loss，计算公式如下。**</p>
<p><strong>loss &#x3D; (yp - y)2</strong></p>
<p>**　　这个值称之为**<strong>损失</strong>（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。</p>
<p><strong>如果将先前的神经网络预测的矩阵公式带入到 yp 中（因为有 z&#x3D;yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为****损失函数</strong>（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。</p>
<p>**　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于 0 的运算量很大，所以一般来说解决这个优化问题使用的是**<strong>梯度下降</strong>算法。</p>
<p>**梯度下降就是顺着函数 **<strong>下降最快的方向</strong>（即梯度的反方向）一步步走，直到找到函数的最低点（局部最小值或全局最小值）。</p>
<hr>
<h3 id="二、基本思想"><a href="#二、基本思想" class="headerlink" title="二、基本思想"></a>二、基本思想</h3><p><strong>假设有一个损失函数 J(θ)J(\theta)J(θ)，其中 θ\thetaθ 表示模型的参数。</strong> ** 我们想要找到使损失函数最小的参数 θ\thetaθ。**</p>
<p><strong>迭代公式为：</strong></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143508053.png" alt="image-20250818143508053"></p>
<p>**意思就是：**<strong>更新参数 θ 为 旧的 θ 减去学习率乘以梯度</strong>。</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143717968.png" alt="image-20250818143717968"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143819978.png" alt="image-20250818143819978"></p>
<p><strong>梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。</strong></p>
<p><strong>在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用****反向传播</strong>算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。</p>
<p>**　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀 E 代表着相对导数的意思。**</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/1b3510e880dd4843ae54ca80ff0adb0a.png" alt="img"></p>
<p><strong>反向传播（Backpropagation, BP）算法</strong></p>
<p><strong>在神经网络里高效计算梯度的方法</strong>。** ** 可以说：</p>
<ul>
<li><strong>梯度下降</strong> 是“怎么走路” → 不断沿着梯度的反方向更新参数。</li>
<li><strong>反向传播</strong> 是“怎么找到路” → 在复杂的神经网络里快速算出梯度。</li>
</ul>
<p><strong>在神经网络里，参数非常多（权重 www、偏置 bbb）。</strong> ** 我们需要知道每个参数对 <strong><strong>损失函数</strong> 的影响（即梯度），才能用梯度下降更新参数。</strong> ** 反向传播就是用 <strong>链式法则</strong> 把误差从输出层一步步传回输入层，计算所有参数的梯度。</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144205043.png" alt="image-20250818144205043"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144253384.png" alt="image-20250818144253384"></p>
<p><strong>反向传播算法的启示是数学中的****链式法则</strong>。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从 BP 算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。</p>
<p>**　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。**</p>
<p><strong>因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做****泛化</strong>（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有<strong>权重衰减</strong>等。</p>
<h2 id="泛化（Generalization）"><a href="#泛化（Generalization）" class="headerlink" title="泛化（Generalization）"></a>泛化（Generalization）</h2><ul>
<li><strong>问题背景</strong>：我们训练模型的时候，模型是接触到训练数据的。但模型最终要应用在“没见过的新数据”上，比如测试集或真实环境的数据。</li>
<li><strong>泛化能力</strong>：指模型在新数据上的预测效果。如果一个模型在训练集上表现很好，但在测试集上表现很差，就说明 <strong>泛化能力差（过拟合）</strong>。</li>
</ul>
<p><strong>👉 所以目标是：不仅要在训练集上学得好，还要在测试集、真实环境中表现好。</strong></p>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><ul>
<li><strong>概念</strong>：正则化就是一类方法，用来提升模型的泛化能力，避免过拟合。</li>
<li><strong>为什么过拟合</strong>：当神经网络参数太多时，它可能会“死记硬背”训练数据，而不是学到真正有用的规律。</li>
<li><strong>正则化的作用</strong>：通过“限制模型的复杂度”或“增加训练时的约束”，让模型学到更稳定的规律，而不是死记硬背。</li>
</ul>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144807879.png" alt="image-20250818144807879"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145158283.png" alt="image-20250818145158283"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145229147.png" alt="image-20250818145229147"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145248395.png" alt="image-20250818145248395"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818151116266.png" alt="image-20250818151116266"></p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818151133957.png" alt="image-20250818151133957"></p>
<p><strong>在被人摒弃的 10 年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的 Geoffery Hinton 教授。</strong></p>
<p>**　　2006 年，Hinton 在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“**<strong>预训练</strong>”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“<strong>微调</strong>”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“<strong>深度学习</strong>”。</p>
<p>** 　很快，深度学习在语音识别领域暂露头角。接着，2012 年，深度学习技术又在图像识别领域大展拳脚。Hinton 与他的学生在 ImageNet 竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率 15% 的好成绩，这个成绩比第二名高了近 11 个百分点，充分证明了多层神经网络识别效果的优越性。**</p>
<p>**　　在这之后，关于深度神经网络的研究与应用不断涌现。**</p>
<p><strong>我们延续两层神经网络的方式来设计一个多层神经网络。</strong></p>
<p>**　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。**</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/446beb41a6c3180cbac3b9907e1d00b2.png" alt="img"></p>
<p><strong>多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。</strong></p>
<p>**　　下面讨论一下多层神经网络中的参数。**</p>
<p>**　　首先我们看第一张图，可以看出**<strong>W</strong>(1)中有 6 个参数，<strong>W</strong>(2)中有 4 个参数，<strong>W</strong>(3)中有 6 个参数，所以整个神经网络中的参数有 16 个（这里我们不考虑偏置节点，下同）。</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/5b22532fd29fc20b299488fa55e22890.png" alt="img"></p>
<p><strong>假设我们将中间层的节点数做一下调整。第一个中间层改为 3 个单元，第二个中间层改为 4 个单元。</strong></p>
<p>**　　经过调整以后，整个网络的参数变成了 33 个。**</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/66c2a2d5e23e80feed37dd9ae11bc2dd.png" alt="img"></p>
<p><strong>虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。</strong></p>
<p>**　　在参数一致的情况下，我们也可以获得一个“更深”的网络。**</p>
<p><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/24a14120a978e9a49ce2b0a9de6ba895.png" alt="img"></p>
<p><strong>图 33 多层神经网络（更深的层次）</strong></p>
<hr>
<p>**　　上图的网络中，虽然参数数量仍然是 33，但却有 4 个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。**</p>
<p>**　　**<strong>3.效果</strong></p>
<p>**　　与两层层神经网络不同。多层神经网络中的层数增加了很多。**</p>
<p>**　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。**</p>
<p>**　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。**</p>
<p>**　　关于逐层特征学习的例子，可以参考下图。**<img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/f10bcb5f682714d1b388506514784550.png" alt="img"></p>
<p><strong>在单层神经网络时，我们使用的激活函数是 sgn 函数。到了两层神经网络时，我们使用的最多的是 sigmoid 函数。而到了多层神经网络时，通过一系列的研究发现，ReLU 函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是 ReLU 函数。ReLU 函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是 y&#x3D;max(x,0)。简而言之，在 x 大于 0，输出就是输入，而在 x 小于 0 时，输出就保持为 0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。</strong></p>
<p>**　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如 GPU 图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　**</p>
<p>**　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**<strong>过拟合现象</strong>。因此正则化技术就显得十分重要。目前，Dropout 技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/zjncs">Johnny-Zhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://zjncs.github.io/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">https://zjncs.github.io/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://opensource.org/licenses/MIT" target="_blank">MIT License</a> 许可协议。转载请注明来源 <a href="https://zjncs.github.io" target="_blank">Johnny-Zhao's TechBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2508/">2508</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar-geek.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/18/llm%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86-RNN/" title="llm前置知识-RNN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">llm前置知识-RNN</div></div><div class="info-2"><div class="info-item-1">循环神经网络（Rerrent Neural Network, RNN） **是神经网络的一种，****RNN 对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，**利用了 RNN 的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等 NLP 领域的问题时...</div></div></div></a><a class="pagination-related" href="/2025/08/14/llm-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="llm-神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">llm-神经网络</div></div><div class="info-2"><div class="info-item-1">由于实习等一系列事情，leetcode 更新暂时延缓，只保留每日刷一道题，知识点学习暂且搁置 下面是 llm 前置知识，在这部分结束后，我将更新经典论文阅读。 神经网络： 让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是****输入层，绿色的是输出层，紫色的是中间层（也叫...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/18/llm%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86-RNN/" title="llm前置知识-RNN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-18</div><div class="info-item-2">llm前置知识-RNN</div></div><div class="info-2"><div class="info-item-1">循环神经网络（Rerrent Neural Network, RNN） **是神经网络的一种，****RNN 对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，**利用了 RNN 的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等 NLP 领域的问题时...</div></div></div></a><a class="pagination-related" href="/2025/08/05/Blog%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97/" title="Blog搭建指南"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-05</div><div class="info-item-2">Blog搭建指南</div></div><div class="info-2"><div class="info-item-1">这篇文章将手把手记录我如何搭建自己的技术博客，涵盖 Hexo 博客初始化、主题美化、内容管理到自动部署的完整流程，希望对也想打造自己博客的你有所帮助。  一、Hexo 初始化与基础配置Hexo 是一款基于 Node.js 的静态博客框架，轻量、快速，非常适合开发者记录技术文章。 初始化项目结构...</div></div></div></a><a class="pagination-related" href="/2025/08/14/llm-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="llm-神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="info-item-2">llm-神经网络</div></div><div class="info-2"><div class="info-item-1">由于实习等一系列事情，leetcode 更新暂时延缓，只保留每日刷一道题，知识点学习暂且搁置 下面是 llm 前置知识，在这部分结束后，我将更新经典论文阅读。 神经网络： 让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是****输入层，绿色的是输出层，紫色的是中间层（也叫...</div></div></div></a><a class="pagination-related" href="/2025/08/28/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0/" title="量子计算学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-28</div><div class="info-item-2">量子计算学习</div></div><div class="info-2"><div class="info-item-1">1.1 量子计算原理 量子比特概念介绍 量子计算是一种基于量子力学基本原理的信息处理范式，它利用量子叠加、量子纠缠和量子干涉等现象，解决经典计算机无法高效处理的问题。 在经典计算机中，信息的基本单元是比特（bit），只以 0 和 1 两种可能的形式存储信息。而在量子计算中，基本单元是量子比特（...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAiIGhlaWdodD0iMTAiIHZpZXdCb3g9IjAgMCAxMCAxMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBmaWxsPSIjMzMzIi8+PC9zdmc+" data-lazy-src="/img/avatar-geek.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Johnny-Zhao</div><div class="author-info-description">技术博客，记录学习与生活点滴</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zjncs"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/zjncs" target="_blank" title="GitHub"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://twitter.com/" target="_blank" title="Twitter"><i class="fab fa-twitter" style="color: #1da1f2;"></i></a><a class="social-icon" href="https://stackoverflow.com" target="_blank" title="Stack Overflow"><i class="fab fa-stack-overflow" style="color: #f48024;"></i></a><a class="social-icon" href="mailto:zhaojianing@tju.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #00ff7f;"></i></a><a class="social-icon" href="https://linkedin.com/in/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin" style="color: #0077b5;"></i></a><a class="social-icon" href="https://dev.to/" target="_blank" title="Dev.to"><i class="fab fa-dev" style="color: #0a0a0a;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">1.</span> <span class="toc-text">二、基本思想</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%EF%BC%88Generalization%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">泛化（Generalization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">正则化（Regularization）</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/28/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97%E5%AD%A6%E4%B9%A0/" title="量子计算学习">量子计算学习</a><time datetime="2025-08-28T10:06:07.000Z" title="发表于 2025-08-28 18:06:07">2025-08-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/18/llm%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86-RNN/" title="llm前置知识-RNN">llm前置知识-RNN</a><time datetime="2025-08-18T08:44:31.000Z" title="发表于 2025-08-18 16:44:31">2025-08-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/18/llm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="llm神经网络深度学习">llm神经网络深度学习</a><time datetime="2025-08-18T07:21:41.000Z" title="发表于 2025-08-18 15:21:41">2025-08-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/14/llm-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="llm-神经网络">llm-神经网络</a><time datetime="2025-08-14T05:47:33.000Z" title="发表于 2025-08-14 13:47:33">2025-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/14/%E8%8A%AF%E7%89%87%E9%AA%8C%E8%AF%81%EF%BC%88%E4%B8%80%EF%BC%89/" title="芯片验证（一）">芯片验证（一）</a><time datetime="2025-08-14T02:10:06.000Z" title="发表于 2025-08-14 10:10:06">2025-08-14</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Johnny-Zhao</span></div><div class="footer_custom_text"><code style="color: #00ff7f;">while(alive) { eat(); sleep(); code(); repeat(); }</code></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'zjncs/zjn-blog-comments',
      'data-repo-id': 'R_kgDOPVdIdg',
      'data-category-id': 'DIC_kwDOPVdIds4CtlzW',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !true) {
    if (true) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script src="/js/matrix-rain.js"></script><script src="/js/geek-enhancements.js"></script><script>
// 添加终端风格的打字声音效果
document.addEventListener('DOMContentLoaded', () => {
  // 代码块展开/折叠动画
  document.querySelectorAll('.code-header .toggle').forEach(btn => {
    btn.addEventListener('click', function() {
      const codeBlock = this.closest('.highlight').querySelector('pre');
      codeBlock.style.transition = 'max-height 0.3s ease';
    });
  });

  // 为链接添加终端点击效果
  document.querySelectorAll('a').forEach(link => {
    link.addEventListener('click', function(e) {
      if (!this.href.startsWith('#') && !e.ctrlKey && !e.metaKey) {
        const clickEffect = document.createElement('span');
        clickEffect.style.position = 'absolute';
        clickEffect.style.width = '20px';
        clickEffect.style.height = '20px';
        clickEffect.style.border = '2px solid #00ff7f';
        clickEffect.style.borderRadius = '50%';
        clickEffect.style.pointerEvents = 'none';
        clickEffect.style.transform = 'translate(-50%, -50%)';
        clickEffect.style.animation = 'pulse 0.6s ease-out forwards';
        clickEffect.style.zIndex = '9999';
        
        const rect = this.getBoundingClientRect();
        const x = rect.left + rect.width / 2;
        const y = rect.top + rect.height / 2;
        
        clickEffect.style.left = `${x}px`;
        clickEffect.style.top = `${y}px`;
        
        document.body.appendChild(clickEffect);
        
        setTimeout(() => {
          clickEffect.remove();
        }, 600);
      }
    });
  });

  // 添加终端风格的加载提示
  window.addEventListener('pjax:send', () => {
    const loader = document.createElement('div');
    loader.id = 'terminal-loader';
    loader.style.position = 'fixed';
    loader.style.top = '50%';
    loader.style.left = '50%';
    loader.style.transform = 'translate(-50%, -50%)';
    loader.style.zIndex = '9999';
    loader.style.backgroundColor = 'rgba(10, 10, 10, 0.8)';
    loader.style.padding = '20px';
    loader.style.borderRadius = '8px';
    loader.style.border = '1px solid #333';
    loader.style.fontFamily = '"JetBrains Mono", monospace';
    loader.style.color = '#00ff7f';
    loader.innerHTML = 'Loading... <span class="terminal-cursor">█</span>';
    document.body.appendChild(loader);
  });

  window.addEventListener('pjax:complete', () => {
    document.getElementById('terminal-loader')?.remove();
  });
});

// 添加CSS动画
const style = document.createElement('style');
style.textContent = `
  @keyframes pulse {
    0% {
      transform: translate(-50%, -50%) scale(0.5);
      opacity: 1;
    }
    100% {
      transform: translate(-50%, -50%) scale(2);
      opacity: 0;
    }
  }
  
  .terminal-cursor::after {
    content: '█';
    animation: blink 1s infinite;
  }
  
  @keyframes blink {
    0%, 50% { opacity: 1; }
    51%, 100% { opacity: 0; }
  }
`;
document.head.appendChild(style);
</script>
<script id="canvas_nest" defer="defer" color="0,255,127" opacity="0.3" zIndex="-1" count="80" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:description\"]","link[rel=\"canonical\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"]):not([href="/tools/"]):not([href="/admin/"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>