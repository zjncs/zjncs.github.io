---
title: llm-神经网络
author: Johnny-Zhao
tags:
  - '2508'
categories:
  - llm+
date: 2025-08-14 13:47:33
---
**由于实习等一系列事情，leetcode 更新暂时延缓，只保留每日刷一道题，知识点学习暂且搁置**

**下面是 llm 前置知识，在这部分结束后，我将更新经典论文阅读。**

**神经网络：**

**让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是****输入层**，绿色的是**输出层**，紫色的是**中间层**（也叫**隐藏层**）。输入层有 3 个输入单元，隐藏层有 4 个单元，输出层有 2 个单元。

![img](https://i-blog.csdnimg.cn/blog_migrate/ec01486fd81a42733d3acef52a95c907.png)

**在开始介绍前，有一些知识可以先记在心里：**

1. **设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；**
2. **神经网络结构图中的拓扑与箭头代表着****预测**过程时数据的流向，跟**训练**时的数据流有一定的区别；
3. **结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的****权重**（其值称为**权值**），这是需要训练得到的。

**除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：**

![img](https://i-blog.csdnimg.cn/blog_migrate/b65bc032792c3cc4e8949dc2b066ac93.png)

**神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。**

**　　下图是一个典型的神经元模型：包含有 3 个输入，1 个输出，以及 2 个计算功能。**

**　　注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。**

![img](https://i-blog.csdnimg.cn/blog_migrate/75b9760fdd6dc27daf143454b9749fd1.png)

**图 6 神经元模型 **

**　　连接是神经元中最重要的东西。每一个连接上都有一个权重。**

**　　一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。**

**　　我们使用 a 来表示输入，用 w 来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是 a，端中间有加权参数 w，经过这个加权后的信号会变成 a**\***w，因此在连接的末端，信号的大小就变成了 a\*w。**

**如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/5edde0590ebecbb366852259ce371054.png)

**可见 z 是在输入和权值的线性加权和叠加了一个****函数 g** 的值。在 MP 模型里，函数 g 是 sgn 函数，也就是取符号函数。这个函数当输入大于 0 时，输出 1，否则输出 0。

**下面对神经元模型的图进行一些扩展。首先将 sum 函数与 sgn 函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入 a 与输出 z 写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。**

**　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。**

![img](https://i-blog.csdnimg.cn/blog_migrate/1159ca7927d03a090e353bf2377deb02.png)

**图 9 神经元扩展 **

**神经元模型的使用可以这样理解：**

**　　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性****预测**未知属性。

**　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是 a1，a2，a3，未知属性的值是 z。z 可以通过公式计算出来。**

**　　这里，已知的属性称之为****特征**，未知的属性称之为**目标**。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值 w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。

**这里虽然简单，但已经建立了神经网络大厦的地基。但是，MP 模型中，权重的值都是预先设置的，因此不能学习。**

**单层神经网络（感知机）**

**　在原来 MP 模型的“输入”位置添加神经元节点，标志其为“输入单元”。**

![img](https://i-blog.csdnimg.cn/blog_migrate/938b7c669240a4250c46f46cde645aab.png)

**在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。**

**我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。我们根据计算层的数量来命名。**

**因此感知机是单层神经网络。**

**假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。**

**　　下图显示了带有两个输出单元的单层神经网络，其中输出单元 z1 的计算公式如下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/53c8c4855619cd0dc1515864c7eb00fe.png)

**图 13 单层神经网络(Z1)**

![image-20250814110911355](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814110911355.png)

**可以看到，z2 的计算中除了三个新的权值：w4，w5，w6 以外，其他与 z1 是一样的。**

**　　整个网络的输出如下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/24e9956cc367ed4e8d21d8d39394ec2a.png)

**图 15 单层神经网络(Z1 和 Z2)**

**　　目前的表达公式有一点不让人满意的就是：w4，w5，w6 是后来加的，很难表现出跟原先的 w1，w2，w3 的关系。**

**目前的表达公式有一点不让人满意的就是：w4，w5，w6 是后来加的，很难表现出跟原先的 w1，w2，w3 的关系。**

**　　因此我们改用二维的下标，用 wx,y 来表达一个权值。下标中的 x 代表后一层神经元的序号，而 y 代表前一层神经元的序号（序号的顺序从上到下）。**

**　　例如，w1,2 代表后一层的第 1 个神经元与前一层的第 2 个神经元的连接的权值（这种标记方式参照了 Andrew Ng 的课件）。根据以上方法标记，我们有了下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/8d37f29e30331a1367b9c4156f9ba1a9.png)

**图 16 单层神经网络(扩展)**

** 如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。**

**　　例如，输入的变量是[a1，a2，a3]T（代表由 a1，a2，a3 组成的列向量），用向量****a** 来表示。方程的左边是[z1，z2]T，用向量 **z** 来表示。

**　　系数则是矩阵****W**（2 行 3 列的矩阵，排列形式与公式中的一样）。

**　　于是，输出公式可以改写成：**

**								**g(**W** \* **a**) = **z**;

**　　这个公式就是神经网络中从前一层计算后一层的****矩阵运算。**

**3.效果**

**　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个****逻辑回归**模型，可以做线性分类任务。

**　　我们可以用****决策分界**来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是 3 维的时候，就是划出一个平面，当数据的维度是 n 维时，就是划出一个 n-1 维的超平面。

**　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。**

![img](https://i-blog.csdnimg.cn/blog_migrate/d8ac48846e6e18911b3a9869d7be5301.png)

**图 17 单层神经网络（决策分界）**

**权值 w 并不是事先人为设定好的，而是模型****根据数据自动学出来的参数**，就像学生通过做题不断修正自己的解题方法。

![image-20250814111819961](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814111819961.png)

![image-20250814111834727](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814111834727.png)

**偏置就是一个额外的权重，对应输入恒为 1，用来调整神经元的输出基准，让模型的决策边界更灵活。**

![image-20250814112554503](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814112554503.png)

**感知器能做的事情：**

* **它可以把空间分成两部分。**
* **所以它只能处理****线性可分**问题（比如 AND、OR）。

**XOR 不满足线性可分条件，因此单层感知器无法找到一组权重 w 和偏置 b 使得输出完全正确。**

![image-20250814113057424](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814113057424.png)

**四. 两层神经网络（多层感知器）**

**　　****1.引子**

**　　两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。**

**　　Minsky 说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。**

**　　1986 年，Rumelhar 和 Hinton 等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 **

**　　这时候的 Hinton 还很年轻，30 年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。**

![img](https://i-blog.csdnimg.cn/blog_migrate/bd5d018f58f101c7b8341a0bc996cc1c.png)

![image-20250814113925679](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814113925679.png)

**　计算最终输出 z 的方式是利用了中间层的 a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/ff6b58c6dc41f9f439a8ceaa9393757c.png)

**图 21 两层神经网络（输出层计算）**

**　　假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。**

**　　我们使用向量和矩阵来表示层次中的变量。****a**(1)，**a**(2)，**z** 是网络中传输的向量数据。**W**(1)和 **W**(2)是网络的矩阵参数。如下图。

![img](https://i-blog.csdnimg.cn/blog_migrate/7b83ef7d2f2eeb3be332a0f60bc1509c.png)

**图 22 两层神经网络（向量形式）**

**　　使用矩阵运算来表达整个计算公式的话如下：**

** g(****W**(1) \* **a**(1)) = **a**(2);

**g(****W**(2) \* **a**(2)) = **z**;

** 需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为 1 的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。**

**　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量****b**，称之为偏置。如下图。

![img](https://i-blog.csdnimg.cn/blog_migrate/c2c8420cff26253a2a0ff71ed9b9ab2c.png)

**图 23 两层神经网络（考虑偏置节点）**

---

**　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 **

**　　在考虑了偏置以后的一个神经网络的矩阵运算如下：**

** g(****W**(1) \* **a**(1) + **b**(1)) = **a**(2);

**g(****W**(2) \* **a**(2) + **b**(2)) = **z**;

---

**　　需要说明的是，在两层神经网络中，我们不再使用 sgn 函数作为函数 g，而是使用平滑函数 sigmoid 作为函数 g。我们把函数 g 也称作激活函数（active function）。**

![image-20250814131412701](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250814131412701.png)

**3.效果**

**　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。**

**　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。**

**　　下面就是一个例子（此两图来自 colah 的**[博客](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。

![img](https://i-blog.csdnimg.cn/blog_migrate/05fc87cd4485eb8a9291a26812596766.png)

**图 24 两层神经网络（决策分界）**

**　　**

**　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？**

**　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。**

### 第二步：隐藏层的“魔术”—— 空间变换 (Spatial Transformation)

**现在，我们在输入层和输出层之间加入一个“隐藏层”，就构成了两层神经网络。这个隐藏层究竟做了什么？**

**它对原始数据的坐标空间进行了一次非线性变换。**

**这个变换可以想象成将原始的坐标纸（特征空间）进行“拉伸”、“弯曲”、“折叠”，使得原本挤在一起、无法用直线分开的数据点，在一个新的坐标系下，变得可以用直线轻易分开了。**

**这个变换具体是通过两个操作完成的：**

1. **线性计算**：输入数据与隐藏层的权重矩阵 (W) 进行矩阵乘法，再加上偏置 (b)。这本质上是对坐标空间进行旋转、缩放、平移等线性变换。
2. **非线性激活**：对上一步的线性计算结果，施加一个**非线性激活函数**（如 Sigmoid, ReLU, Tanh 等）。**这是实现非线性变换的关键！** 如果没有这个非线性激活函数，那么无论多少个隐藏层叠加，最终都等价于一个单层的线性模型，无法学习到非线性的关系。

![img](https://i-blog.csdnimg.cn/blog_migrate/c2a4059fd628588ad7f2a5d4aa60758b.png)

**图中的灰色网格线，实际上就是原始空间中标准的直线网格（比如 x=0.1, 0.2... 和 y=0.1, 0.2...）经过隐藏层****变换后**的样子。您可以看到，原本笔直的网格线被“掰弯”了。这直观地展示了空间本身被扭曲了。

**可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。**
