---
title: llm神经网络深度学习
author: Johnny-Zhao
tags:
  - 论文
  - '2508'
categories:
  - llm+
date: 2025-08-18 15:21:41
---
**下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做 Grid Search（网格搜索）。**

**　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如 EasyPR 字符识别网络架构（下图）。**![img](https://i-blog.csdnimg.cn/blog_migrate/1d3e42d157b2d6714982495a97feb882.png)

**下面简单介绍一下两层神经网络的训练。**

**　　在 Rosenblat 提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000 左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。**

**　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为 yp，真实目标为 y。那么，定义一个值 loss，计算公式如下。**

**loss = (yp - y)2**

**　　这个值称之为****损失**（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。

**如果将先前的神经网络预测的矩阵公式带入到 yp 中（因为有 z=yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为****损失函数**（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。

**　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于 0 的运算量很大，所以一般来说解决这个优化问题使用的是****梯度下降**算法。

**梯度下降就是顺着函数 ****下降最快的方向**（即梯度的反方向）一步步走，直到找到函数的最低点（局部最小值或全局最小值）。

---

### 二、基本思想

**假设有一个损失函数 J(θ)J(\\theta)J(θ)，其中 θ\\thetaθ 表示模型的参数。** ** 我们想要找到使损失函数最小的参数 θ\\thetaθ。**

**迭代公式为：**

![image-20250818143508053](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143508053.png)

**意思就是：****更新参数 θ 为 旧的 θ 减去学习率乘以梯度**。

![image-20250818143717968](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143717968.png)

![image-20250818143819978](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818143819978.png)

**梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。**

**在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用****反向传播**算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

**　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀 E 代表着相对导数的意思。**

![img](https://i-blog.csdnimg.cn/blog_migrate/1b3510e880dd4843ae54ca80ff0adb0a.png)

**反向传播（Backpropagation, BP）算法**

**在神经网络里高效计算梯度的方法**。** ** 可以说：

* **梯度下降** 是“怎么走路” → 不断沿着梯度的反方向更新参数。
* **反向传播** 是“怎么找到路” → 在复杂的神经网络里快速算出梯度。

**在神经网络里，参数非常多（权重 www、偏置 bbb）。** ** 我们需要知道每个参数对 ****损失函数** 的影响（即梯度），才能用梯度下降更新参数。** ** 反向传播就是用 **链式法则** 把误差从输出层一步步传回输入层，计算所有参数的梯度。

![image-20250818144205043](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144205043.png)

![image-20250818144253384](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144253384.png)

**反向传播算法的启示是数学中的****链式法则**。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从 BP 算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

**　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。**

**因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做****泛化**（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有**权重衰减**等。

## 泛化（Generalization）

* **问题背景**：我们训练模型的时候，模型是接触到训练数据的。但模型最终要应用在“没见过的新数据”上，比如测试集或真实环境的数据。
* **泛化能力**：指模型在新数据上的预测效果。如果一个模型在训练集上表现很好，但在测试集上表现很差，就说明 **泛化能力差（过拟合）**。

**👉 所以目标是：不仅要在训练集上学得好，还要在测试集、真实环境中表现好。**

## 正则化（Regularization）

* **概念**：正则化就是一类方法，用来提升模型的泛化能力，避免过拟合。
* **为什么过拟合**：当神经网络参数太多时，它可能会“死记硬背”训练数据，而不是学到真正有用的规律。
* **正则化的作用**：通过“限制模型的复杂度”或“增加训练时的约束”，让模型学到更稳定的规律，而不是死记硬背。

![image-20250818144807879](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818144807879.png)

![image-20250818145158283](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145158283.png)

![image-20250818145229147](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145229147.png)

![image-20250818145248395](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818145248395.png)

![image-20250818151116266](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818151116266.png)

![image-20250818151133957](https://raw.githubusercontent.com/zjncs/TyporaPic/main/imaimage-20250818151133957.png)

**在被人摒弃的 10 年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的 Geoffery Hinton 教授。**

**　　2006 年，Hinton 在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“****预训练**”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“**微调**”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“**深度学习**”。

** 　很快，深度学习在语音识别领域暂露头角。接着，2012 年，深度学习技术又在图像识别领域大展拳脚。Hinton 与他的学生在 ImageNet 竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率 15% 的好成绩，这个成绩比第二名高了近 11 个百分点，充分证明了多层神经网络识别效果的优越性。**

**　　在这之后，关于深度神经网络的研究与应用不断涌现。**

**我们延续两层神经网络的方式来设计一个多层神经网络。**

**　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。**

![img](https://i-blog.csdnimg.cn/blog_migrate/446beb41a6c3180cbac3b9907e1d00b2.png)

**多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。**

**　　下面讨论一下多层神经网络中的参数。**

**　　首先我们看第一张图，可以看出****W**(1)中有 6 个参数，**W**(2)中有 4 个参数，**W**(3)中有 6 个参数，所以整个神经网络中的参数有 16 个（这里我们不考虑偏置节点，下同）。

![img](https://i-blog.csdnimg.cn/blog_migrate/5b22532fd29fc20b299488fa55e22890.png)

**假设我们将中间层的节点数做一下调整。第一个中间层改为 3 个单元，第二个中间层改为 4 个单元。**

**　　经过调整以后，整个网络的参数变成了 33 个。**

![img](https://i-blog.csdnimg.cn/blog_migrate/66c2a2d5e23e80feed37dd9ae11bc2dd.png)

**虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。**

**　　在参数一致的情况下，我们也可以获得一个“更深”的网络。**

![img](https://i-blog.csdnimg.cn/blog_migrate/24a14120a978e9a49ce2b0a9de6ba895.png)

**图 33 多层神经网络（更深的层次）**

---

**　　上图的网络中，虽然参数数量仍然是 33，但却有 4 个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。**

**　　****3.效果**

**　　与两层层神经网络不同。多层神经网络中的层数增加了很多。**

**　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。**

**　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。**

**　　关于逐层特征学习的例子，可以参考下图。**![img](https://i-blog.csdnimg.cn/blog_migrate/f10bcb5f682714d1b388506514784550.png)

**在单层神经网络时，我们使用的激活函数是 sgn 函数。到了两层神经网络时，我们使用的最多的是 sigmoid 函数。而到了多层神经网络时，通过一系列的研究发现，ReLU 函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是 ReLU 函数。ReLU 函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是 y=max(x,0)。简而言之，在 x 大于 0，输出就是输入，而在 x 小于 0 时，输出就保持为 0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。**

**　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如 GPU 图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　**

**　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现****过拟合现象**。因此正则化技术就显得十分重要。目前，Dropout 技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。
